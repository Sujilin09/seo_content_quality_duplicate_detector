{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42deae7c",
   "metadata": {},
   "source": [
    "STEP  1: Data Collection & HTML Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c88e621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 0. Imports ---\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ef9afb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data path: ../data/data.csv\n",
      "Output path: ../data/extracted_content.csv\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Configuration & Setup ---\n",
    "# Define file paths. '../' means 'go up one directory'\n",
    "# We are in 'notebooks/', so we go up to 'seo-content-detector/' then down to 'data/'\n",
    "DATA_DIR = '../data/'\n",
    "RAW_DATA_PATH = os.path.join(DATA_DIR, 'data.csv')\n",
    "EXTRACTED_DATA_PATH = os.path.join(DATA_DIR, 'extracted_content.csv')\n",
    "\n",
    "print(f\"Raw data path: {RAW_DATA_PATH}\")\n",
    "print(f\"Output path: {EXTRACTED_DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5afab76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully loaded ../data/data.csv with 81 rows.\n",
      "Columns found: ['url', 'html_content']\n",
      "                                                 url  \\\n",
      "0     https://www.cm-alliance.com/cybersecurity-blog   \n",
      "1    https://www.varonis.com/blog/cybersecurity-tips   \n",
      "2  https://www.cisecurity.org/insights/blog/11-cy...   \n",
      "3  https://www.cisa.gov/topics/cybersecurity-best...   \n",
      "4  https://www.qnbtrust.bank/Resources/Learning-C...   \n",
      "\n",
      "                                        html_content  \n",
      "0  <!doctype html><!--[if lt IE 7]> <html class=\"...  \n",
      "1  <!doctype html><html lang=\"en\"><head>\\n    <me...  \n",
      "2  <!DOCTYPE html><html data-unhead-vue-server-re...  \n",
      "3  \\n\\n<!DOCTYPE html>\\n<html lang=\"en\" dir=\"ltr\"...  \n",
      "4                                                NaN  \n"
     ]
    }
   ],
   "source": [
    "# --- 2. Data Loading ---\n",
    "try:\n",
    "    df = pd.read_csv(RAW_DATA_PATH)\n",
    "    print(f\"\\nSuccessfully loaded {RAW_DATA_PATH} with {len(df)} rows.\")\n",
    "    print(\"Columns found:\", df.columns.tolist())\n",
    "    print(df.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Could not find data.csv at {RAW_DATA_PATH}\")\n",
    "    print(\"Please make sure you have downloaded the dataset and placed it in the 'data/' folder.\")\n",
    "# Stop execution if the file isn't found\n",
    "if 'df' not in locals():\n",
    "    raise SystemExit(\"Stopping execution: Data file not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565fdbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. HTML Parsing Function ---\n",
    "\n",
    "def parse_html_content(html_content):\n",
    "    \"\"\"\n",
    "    Parses raw HTML to extract title, clean body text, and word count.\n",
    "    \n",
    "    Args:\n",
    "        html_content (str): The raw HTML string.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (title, body_text, word_count)\n",
    "    \"\"\"\n",
    "    # Handle non-string (e.g., NaN, float) inputs gracefully\n",
    "    if not isinstance(html_content, str):\n",
    "        return \"\", \"\", 0\n",
    "\n",
    "    try:\n",
    "        # Use 'lxml' for a fast and forgiving HTML parser\n",
    "        soup = BeautifulSoup(html_content, 'lxml')\n",
    "\n",
    "        # 3a. Extract Title\n",
    "        title = soup.title.string if soup.title else \"No Title Found\"\n",
    "        \n",
    "        # 3b. Extract Main Body Text\n",
    "        # This is a heuristic (educated guess) approach. We prioritize tags \n",
    "        # that usually contain the main article.\n",
    "        \n",
    "        body_text = \"\"\n",
    "        \n",
    "        # Try to find the main content in specific tags\n",
    "        if soup.find('article'):\n",
    "            main_content = soup.find('article')\n",
    "        elif soup.find('main'):\n",
    "            main_content = soup.find('main')\n",
    "        else:\n",
    "            # Fallback to the whole body if no <article> or <main>\n",
    "            main_content = soup.find('body')\n",
    "\n",
    "        # If we found a content block, get text from it\n",
    "        if main_content:\n",
    "            # Get text from all <p> (paragraph) tags inside the main content\n",
    "            paragraphs = main_content.find_all('p')\n",
    "            if paragraphs:\n",
    "                body_text = \" \".join([p.get_text() for p in paragraphs])\n",
    "            else:\n",
    "                # If no <p> tags, just get all text from the main block\n",
    "                body_text = main_content.get_text()\n",
    "        else:\n",
    "            # Absolute fallback: just get all text from the page\n",
    "            body_text = soup.get_text()\n",
    "\n",
    "        # 3c. Clean the extracted text\n",
    "        # Remove extra whitespace, tabs, and newlines\n",
    "        body_text = re.sub(r'\\s+', ' ', body_text).strip()\n",
    "        \n",
    "        # 3d. Calculate Word Count\n",
    "        word_count = len(body_text.split())\n",
    "\n",
    "        return title, body_text, word_count\n",
    "\n",
    "    except Exception as e:\n",
    "        # Catch any unexpected parsing errors\n",
    "        print(f\"Error parsing content: {e}\")\n",
    "        return \"Parsing Error\", \"\", 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d708d888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting HTML parsing for all rows (this may take a moment)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing HTML:   0%|          | 0/81 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing HTML: 100%|██████████| 81/81 [00:07<00:00, 11.06it/s]\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Apply Parsing to DataFrame ---\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"Parsing HTML\")\n",
    "print(\"\\nStarting HTML parsing for all rows (this may take a moment)...\")\n",
    "\n",
    "# We use .progress_apply() from tqdm to get a progress bar!\n",
    "# This applies our function to every row in the 'html_content' column.\n",
    "parsed_data = df['html_content'].progress_apply(parse_html_content)\n",
    "\n",
    "# The result is a Series of tuples. Let's make it a new DataFrame.\n",
    "df_parsed = pd.DataFrame(parsed_data.tolist(), columns=['title', 'body_text', 'word_count'])\n",
    "\n",
    "# Combine the original 'url' column with our new parsed data\n",
    "df_extracted = pd.concat([df['url'], df_parsed], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1e26a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing complete.\n",
      "Successfully parsed 69 rows with content (dropped 12 empty/failed rows).\n",
      "\n",
      "Successfully saved extracted content to ../data/extracted_content.csv\n",
      "\n",
      "--- Extracted Content (Head) ---\n",
      "                                                 url  \\\n",
      "0     https://www.cm-alliance.com/cybersecurity-blog   \n",
      "1    https://www.varonis.com/blog/cybersecurity-tips   \n",
      "2  https://www.cisecurity.org/insights/blog/11-cy...   \n",
      "3  https://www.cisa.gov/topics/cybersecurity-best...   \n",
      "4  https://nordlayer.com/learn/network-security/b...   \n",
      "\n",
      "                                               title  \\\n",
      "0                                Cyber Security Blog   \n",
      "1  Top 10 Cybersecurity Awareness Tips: How to St...   \n",
      "2  11 Cyber Defense Tips to Stay Secure at Work a...   \n",
      "3  Cybersecurity Best Practices | Cybersecurity a...   \n",
      "4     Network Security 101: Understanding the Basics   \n",
      "\n",
      "                                           body_text  word_count  \n",
      "0  Cyber Crisis Tabletop Exercise Cyber Security ...         326  \n",
      "1  Cybersecurity is gaining more importance globa...        1570  \n",
      "2  Cybersecurity is inextricably tied to the tech...         946  \n",
      "3  Cyberspace is particularly difficult to secure...         489  \n",
      "4  Every week, networks seem to grow in size and ...        1065  \n",
      "\n",
      "--- Extracted Content (Head) ---\n",
      "                                                 url  \\\n",
      "0     https://www.cm-alliance.com/cybersecurity-blog   \n",
      "1    https://www.varonis.com/blog/cybersecurity-tips   \n",
      "2  https://www.cisecurity.org/insights/blog/11-cy...   \n",
      "3  https://www.cisa.gov/topics/cybersecurity-best...   \n",
      "4  https://nordlayer.com/learn/network-security/b...   \n",
      "\n",
      "                                               title  \\\n",
      "0                                Cyber Security Blog   \n",
      "1  Top 10 Cybersecurity Awareness Tips: How to St...   \n",
      "2  11 Cyber Defense Tips to Stay Secure at Work a...   \n",
      "3  Cybersecurity Best Practices | Cybersecurity a...   \n",
      "4     Network Security 101: Understanding the Basics   \n",
      "\n",
      "                                           body_text  word_count  \n",
      "0  Cyber Crisis Tabletop Exercise Cyber Security ...         326  \n",
      "1  Cybersecurity is gaining more importance globa...        1570  \n",
      "2  Cybersecurity is inextricably tied to the tech...         946  \n",
      "3  Cyberspace is particularly difficult to secure...         489  \n",
      "4  Every week, networks seem to grow in size and ...        1065  \n"
     ]
    }
   ],
   "source": [
    "# --- 5. Filter & Save Extracted Data ---\n",
    "# Let's check our work and filter out any failed/empty pages\n",
    "original_count = len(df_extracted)\n",
    "# We only want to keep rows where we successfully extracted text\n",
    "df_extracted = df_extracted[df_extracted['word_count'] > 0].reset_index(drop=True)\n",
    "print(f\"\\nParsing complete.\")\n",
    "print(f\"Successfully parsed {len(df_extracted)} rows with content (dropped {original_count - len(df_extracted)} empty/failed rows).\")\n",
    "\n",
    "# Save the cleaned data to the 'data/' folder\n",
    "try:\n",
    "    df_extracted.to_csv(EXTRACTED_DATA_PATH, index=False)\n",
    "    print(f\"\\nSuccessfully saved extracted content to {EXTRACTED_DATA_PATH}\")\n",
    "    \n",
    "    # Display the final cleaned data\n",
    "    print(\"\\n--- Extracted Content (Head) ---\")\n",
    "    print(df_extracted.head())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error saving file: {e}\")\n",
    "\n",
    "# Display the final cleaned data\n",
    "print(\"\\n--- Extracted Content (Head) ---\")\n",
    "print(df_extracted.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c06490",
   "metadata": {},
   "source": [
    "STEP 2: Text Preprocessing & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2c8f56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4edbdc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aee24f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54ff096",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db23dce6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spaCy)",
   "language": "python",
   "name": "spacy_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
